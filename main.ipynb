{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ee97a0fd-b135-4c54-81d6-a5f29962ff24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pybaseball\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date, timedelta, datetime, timezone\n",
    "import pytz\n",
    "import time\n",
    "import logging\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import List, Dict, Tuple, Callable, Optional\n",
    "from sklearn.preprocessing import StandardScaler # Or other scaler\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import importlib\n",
    "import src.pybaseball_dataset as pybaseball_dataset  # Import the module with an alias\n",
    "import src.pytorch_dataset as pytorch_dataset      # Import the module with an alias\n",
    "import src.model as model                          # Import the module with an alias\n",
    "import src.train_eval as train_eval\n",
    "\n",
    "from src.pybaseball_dataset import *\n",
    "from src.pytorch_dataset import *\n",
    "from src.model import *\n",
    "from src.train_eval import *\n",
    "\n",
    "importlib.reload(pybaseball_dataset)\n",
    "importlib.reload(pytorch_dataset)\n",
    "importlib.reload(model)\n",
    "importlib.reload(train_eval)\n",
    "\n",
    "from src.pybaseball_dataset import *\n",
    "from src.pytorch_dataset import *\n",
    "from src.model import *\n",
    "from src.train_eval import *\n",
    "\n",
    "\n",
    "# Setup basic logging\n",
    "log_file = f\"results/{datetime.now().astimezone(pytz.timezone('America/Los_Angeles')).strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "logging.basicConfig(level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                   handlers=[\n",
    "                        logging.FileHandler(log_file),\n",
    "                        logging.StreamHandler()\n",
    "                    ])\n",
    "\n",
    "# Define cache directory (can be passed as argument later)\n",
    "CACHE_DIR = \"dataset\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True) # Create cache directory if it doesn't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f3921fa4-803a-479e-b4bd-4e2fe2cf0d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(\n",
    "    # Data params\n",
    "    start_year: int,\n",
    "    end_year: int,\n",
    "    cache_dir: str = CACHE_DIR,\n",
    "    # Feature params - Define based on preprocess output\n",
    "    target_action_col: str = 'target_action',\n",
    "    target_ev_la_cols: List[str] = ['target_ev', 'target_la'],\n",
    "    bip_action_index: int = 3,\n",
    "    seq_length: int = 10,\n",
    "    # Model Hyperparameters\n",
    "    model_params: Dict = { # Default example parameters\n",
    "        'd_model': 128, 'nhead': 8, 'num_encoder_layers': 4,\n",
    "        'dim_feedforward': 512, 'dropout': 0.1, 'num_mdn_components': 5\n",
    "    },\n",
    "    # Training params\n",
    "    batch_size: int = 64,\n",
    "    num_epochs: int = 20,\n",
    "    learning_rate: float = 1e-4,\n",
    "    val_split_size: float = 0.15,\n",
    "    test_split_size: float = 0.10, # Set to 0 if no test set needed now\n",
    "    device_str: str = 'cuda',\n",
    "    early_stopping_patience: int = 3\n",
    "    ) -> None:\n",
    "    \"\"\"\n",
    "    Orchestrates the entire process: data fetching (with raw cache),\n",
    "    preprocessing (with processed cache), dataset/loader creation,\n",
    "    model initialization, training, and evaluation.\n",
    "    \"\"\"\n",
    "    logging.info(\"Starting training orchestration with caching...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # --- Setup ---\n",
    "    if device_str == 'cuda' and torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        logging.info(\"Using CUDA device.\")\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        logging.info(\"Using CPU device.\")\n",
    "\n",
    "    all_processed_dfs = []\n",
    "    # --- Data Fetching & Preprocessing Loop (with Caching) ---\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        logging.info(f\"\\n--- Processing Year: {year} ---\")\n",
    "        processed_cache_file = os.path.join(cache_dir, f\"processed_statcast_{year}.parquet\")\n",
    "\n",
    "        # Check for processed data cache\n",
    "        if os.path.exists(processed_cache_file):\n",
    "            try:\n",
    "                logging.info(f\"Loading cached processed data for {year} from {processed_cache_file}\")\n",
    "                processed_df = pd.read_parquet(processed_cache_file)\n",
    "                # Basic validation\n",
    "                if 'target_action' in processed_df.columns and 'at_bat_id' in processed_df.columns:\n",
    "                    logging.info(f\"Loaded {len(processed_df)} processed pitches from cache for {year}.\")\n",
    "                    all_processed_dfs.append(processed_df)\n",
    "                    continue # Skip to next year\n",
    "                else:\n",
    "                    logging.warning(f\"Cached processed file {processed_cache_file} invalid. Re-processing.\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error loading processed cache {processed_cache_file}: {e}. Re-processing.\")\n",
    "\n",
    "        # If processed cache doesn't exist or is invalid, process raw data\n",
    "        logging.info(f\"Processing raw data for {year}...\")\n",
    "        # Fetch raw data (uses its own cache, except for current year)\n",
    "        raw_df = fetch_statcast_data_for_year(year, cache_dir)\n",
    "\n",
    "        if raw_df.empty:\n",
    "            logging.warning(f\"No raw data found or fetched for {year}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Preprocess the raw data for the year\n",
    "        processed_df = preprocess_and_feature_engineer(raw_df)\n",
    "\n",
    "        if processed_df.empty:\n",
    "            logging.warning(f\"Preprocessing failed or resulted in empty data for {year}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Save the processed data to cache\n",
    "        try:\n",
    "            logging.info(f\"Saving processed data for {year} to {processed_cache_file}\")\n",
    "            processed_df.to_parquet(processed_cache_file, index=False)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error saving processed data cache file {processed_cache_file}: {e}\")\n",
    "\n",
    "        all_processed_dfs.append(processed_df)\n",
    "\n",
    "    # --- Combine Data and Proceed ---\n",
    "    if not all_processed_dfs:\n",
    "        logging.error(\"No processed data available after checking all years. Exiting.\")\n",
    "        return\n",
    "\n",
    "    logging.info(\"Concatenating processed data from all years...\")\n",
    "    combined_processed_data = pd.concat(all_processed_dfs, ignore_index=True)\n",
    "    logging.info(f\"Total processed pitches combined: {len(combined_processed_data)}\")\n",
    "\n",
    "\n",
    "    # --- Feature Definition (after loading/processing all data) ---\n",
    "    # Re-define feature lists based on the final combined dataframe, just to be safe\n",
    "    numerical_features_base = ['release_speed', 'release_pos_x', 'release_pos_z', 'release_extension', 'release_spin_rate', 'spin_axis', 'pfx_x', 'pfx_z', 'inning', 'outs_when_up', 'score_diff', 'balls', 'strikes']\n",
    "    runner_features = ['on_1b_flag', 'on_2b_flag', 'on_3b_flag']\n",
    "    categorical_features_base = ['pitch_type', 'stand', 'p_throws', 'inning_topbot_numeric']\n",
    "    numerical_features = [f for f in numerical_features_base if f in combined_processed_data.columns] + runner_features\n",
    "    categorical_features = [f for f in categorical_features_base if f in combined_processed_data.columns]\n",
    "    logging.info(f\"Final Numerical Features for Training: {numerical_features}\")\n",
    "    logging.info(f\"Final Categorical Features for Training: {categorical_features}\")\n",
    "    num_actions = combined_processed_data[target_action_col].nunique() # Get num actions from data\n",
    "\n",
    "\n",
    "    # --- Data Splitting (by Game ID on Combined Data) ---\n",
    "    if 'game_pk' not in combined_processed_data.columns:\n",
    "         raise ValueError(\"'game_pk' needed for splitting but not found.\")\n",
    "    game_ids = combined_processed_data['game_pk'].unique()\n",
    "    logging.info(f\"Splitting combined data based on {len(game_ids)} unique games.\")\n",
    "    train_val_splitter = GroupShuffleSplit(n_splits=1, test_size=val_split_size + test_split_size, random_state=42)\n",
    "    train_idx, val_test_idx = next(train_val_splitter.split(game_ids, groups=game_ids))\n",
    "    train_game_ids = game_ids[train_idx]; val_test_game_ids = game_ids[val_test_idx]\n",
    "    if test_split_size > 0 and val_split_size > 0:\n",
    "         relative_test_size = test_split_size / (val_split_size + test_split_size)\n",
    "         val_test_splitter = GroupShuffleSplit(n_splits=1, test_size=relative_test_size, random_state=42)\n",
    "         val_idx_rel, test_idx_rel = next(val_test_splitter.split(val_test_game_ids, groups=val_test_game_ids))\n",
    "         val_game_ids = val_test_game_ids[val_idx_rel]; test_game_ids = val_test_game_ids[test_idx_rel]\n",
    "    elif val_split_size > 0: # Only validation split needed\n",
    "         val_game_ids = val_test_game_ids; test_game_ids = np.array([])\n",
    "    else: # No val or test split (use all for training - not recommended)\n",
    "         train_game_ids = game_ids; val_game_ids = np.array([]); test_game_ids = np.array([])\n",
    "\n",
    "    train_data = combined_processed_data[combined_processed_data['game_pk'].isin(train_game_ids)].copy()\n",
    "    val_data = combined_processed_data[combined_processed_data['game_pk'].isin(val_game_ids)].copy()\n",
    "    test_data = combined_processed_data[combined_processed_data['game_pk'].isin(test_game_ids)].copy()\n",
    "    logging.info(f\"Data split: Train={len(train_data)}, Val={len(val_data)}, Test={len(test_data)}\")\n",
    "\n",
    "\n",
    "    # --- Dataset & DataLoader Creation ---\n",
    "    if train_data.empty or val_data.empty:\n",
    "         logging.error(\"Training or Validation data is empty after split. Cannot proceed.\")\n",
    "         return\n",
    "\n",
    "    train_loader, val_loader, fitted_scaler, fitted_cat_mappings, fitted_cat_vocab_sizes = create_dataloaders(\n",
    "        train_data=train_data, val_data=val_data, numerical_features=numerical_features,\n",
    "        categorical_features=categorical_features, target_action_col=target_action_col,\n",
    "        target_ev_la_cols=target_ev_la_cols, seq_length=seq_length, batch_size=batch_size, device=device )\n",
    "    if not fitted_cat_vocab_sizes: logging.error(\"Cat vocab sizes missing.\"); return\n",
    "\n",
    "\n",
    "    # --- Model Initialization ---\n",
    "    num_numerical = len(numerical_features)\n",
    "    cat_indices = list(range(num_numerical, num_numerical + len(categorical_features)))\n",
    "    # Ensure num_actions derived from data is used if different from default\n",
    "    model_params_updated = model_params.copy()\n",
    "    model_params_updated['num_actions'] = num_actions\n",
    "\n",
    "    model = BatterActionTransformer(\n",
    "        num_numerical_features=num_numerical, cat_feature_indices=cat_indices,\n",
    "        cat_vocab_sizes=fitted_cat_vocab_sizes, seq_length=seq_length,\n",
    "        **model_params_updated # Pass hyperparameters dict\n",
    "    ).to(device)\n",
    "    logging.info(f\"Model initialized ({sum(p.numel() for p in model.parameters() if p.requires_grad):,} params).\")\n",
    "\n",
    "\n",
    "    # --- Optimizer & Loss ---\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = mdn_loss_fn # Use the defined function\n",
    "\n",
    "    # --- Training Loop ---\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_accuracy': [], 'val_mae_ev': [], 'val_mae_la': []}\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        logging.info(f\"\\n--- Epoch {epoch}/{num_epochs} ---\")\n",
    "        train_loss = train_epoch(model, train_loader, loss_fn, optimizer, device, bip_action_index)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        logging.info(f\"Epoch {epoch} Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "        val_metrics = evaluate_model(model, val_loader, loss_fn, device, bip_action_index)\n",
    "        history['val_loss'].append(val_metrics['loss'])\n",
    "        history['val_accuracy'].append(val_metrics['accuracy'])\n",
    "        history['val_mae_ev'].append(val_metrics['mae_ev_bip'])\n",
    "        history['val_mae_la'].append(val_metrics['mae_la_bip'])\n",
    "        logging.info(f\"Epoch {epoch} Val Loss: {val_metrics['loss']:.4f}, Acc: {val_metrics['accuracy']:.4f}, MAE_EV: {val_metrics['mae_ev_bip']:.2f}, MAE_LA: {val_metrics['mae_la_bip']:.2f}\")\n",
    "\n",
    "        if val_metrics['loss'] < best_val_loss:\n",
    "            best_val_loss = val_metrics['loss']\n",
    "            epochs_no_improve = 0\n",
    "            # torch.save(model.state_dict(), os.path.join(cache_dir, 'best_model_checkpoint.pth')) # Save in cache dir\n",
    "            # logging.info(\"Val loss improved, saving model.\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            logging.info(f\"Val loss did not improve for {epochs_no_improve} epoch(s).\")\n",
    "        if epochs_no_improve >= early_stopping_patience:\n",
    "            logging.info(f\"Early stopping triggered after {epoch} epochs.\")\n",
    "            break\n",
    "\n",
    "    # --- Final Steps ---\n",
    "    total_time = time.time() - start_time\n",
    "    logging.info(f\"\\nTraining finished in {total_time / 60:.2f} minutes.\")\n",
    "    logging.info(f\"Best Validation Loss: {best_val_loss:.4f}\")\n",
    "\n",
    "    # --- (Optional) Test Set Evaluation ---\n",
    "    if not test_data.empty:\n",
    "         logging.info(\"\\nEvaluating on Test Set...\")\n",
    "         # Load best model if saved\n",
    "         # if os.path.exists(os.path.join(cache_dir, 'best_model_checkpoint.pth')):\n",
    "         #     model.load_state_dict(torch.load(os.path.join(cache_dir, 'best_model_checkpoint.pth'), map_location=device))\n",
    "\n",
    "         test_dataset = PitchSequenceDataset(\n",
    "              test_data, numerical_features, categorical_features,\n",
    "              target_action_col, target_ev_la_cols, seq_length,\n",
    "              scaler=fitted_scaler, # Use scaler from train\n",
    "              cat_mappings=fitted_cat_mappings, # Use mappings from train\n",
    "              cat_vocab_sizes=fitted_cat_vocab_sizes,\n",
    "              device=device\n",
    "          )\n",
    "         test_loader = DataLoader(test_dataset, batch_size=batch_size*2, shuffle=False)\n",
    "         test_metrics = evaluate_model(model, test_loader, loss_fn, device, bip_action_index)\n",
    "         logging.info(f\"Test Loss: {test_metrics['loss']:.4f}, Acc: {test_metrics['accuracy']:.4f}, MAE_EV: {test_metrics['mae_ev_bip']:.2f}, MAE_LA: {test_metrics['mae_la_bip']:.2f}\")\n",
    "\n",
    "\n",
    "    logging.info(\"run_training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "60b38ef9-e0cb-41f8-aa9d-5dd9d21b4adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 23:05:01,151 - INFO - Starting training orchestration with caching...\n",
      "2025-04-11 23:05:01,154 - INFO - Using CUDA device.\n",
      "2025-04-11 23:05:01,156 - INFO - \n",
      "--- Processing Year: 2022 ---\n",
      "2025-04-11 23:05:01,157 - INFO - Loading cached processed data for 2022 from dataset/processed_statcast_2022.parquet\n",
      "2025-04-11 23:05:01,525 - INFO - Loaded 754158 processed pitches from cache for 2022.\n",
      "2025-04-11 23:05:01,526 - INFO - \n",
      "--- Processing Year: 2023 ---\n",
      "2025-04-11 23:05:01,528 - INFO - Loading cached processed data for 2023 from dataset/processed_statcast_2023.parquet\n",
      "2025-04-11 23:05:01,872 - INFO - Loaded 751775 processed pitches from cache for 2023.\n",
      "2025-04-11 23:05:01,873 - INFO - Concatenating processed data from all years...\n",
      "2025-04-11 23:05:02,094 - INFO - Total processed pitches combined: 1505933\n",
      "2025-04-11 23:05:02,096 - INFO - Final Numerical Features for Training: ['release_speed', 'release_pos_x', 'release_pos_z', 'release_extension', 'release_spin_rate', 'spin_axis', 'pfx_x', 'pfx_z', 'inning', 'outs_when_up', 'score_diff', 'balls', 'strikes', 'on_1b_flag', 'on_2b_flag', 'on_3b_flag']\n",
      "2025-04-11 23:05:02,097 - INFO - Final Categorical Features for Training: ['pitch_type', 'stand', 'p_throws', 'inning_topbot_numeric']\n",
      "2025-04-11 23:05:02,125 - INFO - Splitting combined data based on 5404 unique games.\n",
      "2025-04-11 23:05:02,795 - INFO - Data split: Train=1129031, Val=226181, Test=150721\n",
      "2025-04-11 23:05:02,796 - INFO - Resetting index on training and validation data splits...\n",
      "2025-04-11 23:05:02,986 - INFO - Creating Training Dataset...\n",
      "2025-04-11 23:05:02,987 - INFO - Fitting StandardScaler on numerical features...\n",
      "2025-04-11 23:05:03,995 - INFO - Applied StandardScaler.\n",
      "2025-04-11 23:05:03,996 - INFO - Creating categorical mappings...\n",
      "2025-04-11 23:05:04,045 - INFO -   Mapped 'pitch_type' (17 unique values).\n",
      "2025-04-11 23:05:04,094 - INFO -   Mapped 'stand' (2 unique values).\n",
      "2025-04-11 23:05:04,143 - INFO -   Mapped 'p_throws' (2 unique values).\n",
      "2025-04-11 23:05:04,410 - INFO -   Mapped 'inning_topbot_numeric' (2 unique values).\n",
      "2025-04-11 23:05:04,462 - INFO - Applied mapping for 'pitch_type'.\n",
      "2025-04-11 23:05:04,513 - INFO - Applied mapping for 'stand'.\n",
      "2025-04-11 23:05:04,568 - INFO - Applied mapping for 'p_throws'.\n",
      "2025-04-11 23:05:04,852 - INFO - Applied mapping for 'inning_topbot_numeric'.\n",
      "2025-04-11 23:05:05,010 - INFO - Grouping data by at-bat and creating sequence indices...\n",
      "2025-04-11 23:05:47,895 - INFO - Created 1129031 sequences from 304302 at-bats (1129031 total pitches).\n",
      "2025-04-11 23:05:47,898 - INFO - Creating Validation Dataset...\n",
      "2025-04-11 23:05:47,900 - INFO - Using pre-fitted StandardScaler.\n",
      "2025-04-11 23:05:48,033 - INFO - Applied StandardScaler.\n",
      "2025-04-11 23:05:48,034 - INFO - Using pre-defined categorical mappings.\n",
      "2025-04-11 23:05:48,047 - INFO - Applied mapping for 'pitch_type'.\n",
      "2025-04-11 23:05:48,059 - INFO - Applied mapping for 'stand'.\n",
      "2025-04-11 23:05:48,072 - INFO - Applied mapping for 'p_throws'.\n",
      "2025-04-11 23:05:48,132 - INFO - Applied mapping for 'inning_topbot_numeric'.\n",
      "2025-04-11 23:05:48,154 - INFO - Grouping data by at-bat and creating sequence indices...\n",
      "2025-04-11 23:05:52,875 - INFO - Created 226181 sequences from 60908 at-bats (226181 total pitches).\n",
      "2025-04-11 23:05:52,877 - INFO - Creating DataLoaders with batch_size=128, num_workers=0...\n",
      "2025-04-11 23:05:52,878 - INFO - DataLoaders created.\n",
      "2025-04-11 23:05:52,928 - INFO - Model initialized (801,949 params).\n",
      "2025-04-11 23:05:52,929 - INFO - \n",
      "--- Epoch 1/5 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6760b73c02c0440e8f269154749dfccf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch:   0%|          | 0/8820 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hwen/285_proj_kernel/lib/python3.11/site-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "2025-04-11 23:13:11,130 - INFO - Epoch 1 Train Loss: 12.7456\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "014a557606f54d409050ebcb19dfbd30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/884 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Expected parameter logits (Tensor of shape (41, 5)) of distribution Categorical(logits: torch.Size([41, 5])) to satisfy the constraint IndependentConstraint(Real(), 1), but found invalid values:\ntensor([[nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan]], device='cuda:0')",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Define other model params based on choices above\u001b[39;00m\n\u001b[32m      5\u001b[39m model_constructor_params = {\n\u001b[32m      6\u001b[39m     \u001b[33m'\u001b[39m\u001b[33md_model\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m128\u001b[39m,\n\u001b[32m      7\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mnhead\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m8\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m     \u001b[38;5;66;03m# feature dims/indices/vocabs determined after data loading\u001b[39;00m\n\u001b[32m     15\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[43mrun_training\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstart_year\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2022\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Shortened range for faster testing\u001b[39;49;00m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdataset\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mend_year\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2023\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseq_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_constructor_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Pass dict here (will be updated internally)\u001b[39;49;00m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Increased batch size\u001b[39;49;00m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Reduced epochs for faster testing\u001b[39;49;00m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5e-5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Adjusted learning rate\u001b[39;49;00m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_str\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m     27\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[57]\u001b[39m\u001b[32m, line 175\u001b[39m, in \u001b[36mrun_training\u001b[39m\u001b[34m(start_year, end_year, cache_dir, target_action_col, target_ev_la_cols, bip_action_index, seq_length, model_params, batch_size, num_epochs, learning_rate, val_split_size, test_split_size, device_str, early_stopping_patience)\u001b[39m\n\u001b[32m    172\u001b[39m history[\u001b[33m'\u001b[39m\u001b[33mtrain_loss\u001b[39m\u001b[33m'\u001b[39m].append(train_loss)\n\u001b[32m    173\u001b[39m logging.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m val_metrics = \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbip_action_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m history[\u001b[33m'\u001b[39m\u001b[33mval_loss\u001b[39m\u001b[33m'\u001b[39m].append(val_metrics[\u001b[33m'\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m    177\u001b[39m history[\u001b[33m'\u001b[39m\u001b[33mval_accuracy\u001b[39m\u001b[33m'\u001b[39m].append(val_metrics[\u001b[33m'\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Gen-Batter/src/train_eval.py:101\u001b[39m, in \u001b[36mevaluate_model\u001b[39m\u001b[34m(model, dataloader, loss_fn, device, bip_action_index)\u001b[39m\n\u001b[32m     98\u001b[39m outputs = model(features, src_padding_mask=padding_mask)\n\u001b[32m    100\u001b[39m \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m loss = \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrue_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrue_ev_la\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbip_action_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    102\u001b[39m total_loss += loss.item() * batch_size \u001b[38;5;66;03m# Accumulate total loss, not average\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[38;5;66;03m# Calculate action accuracy\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Gen-Batter/src/model.py:190\u001b[39m, in \u001b[36mmdn_loss_fn\u001b[39m\u001b[34m(y_pred, y_true_action, y_true_ev_la, bip_action_index)\u001b[39m\n\u001b[32m    187\u001b[39m mdn_sigma_bip = torch.clamp(mdn_sigma_bip, \u001b[38;5;28mmin\u001b[39m=min_sigma)\n\u001b[32m    189\u001b[39m \u001b[38;5;66;03m# Create the GMM distribution\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m mix = \u001b[43mD\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCategorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmdn_alpha_logits_bip\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[38;5;66;03m# Using Normal for independent EV/LA components for simplicity\u001b[39;00m\n\u001b[32m    192\u001b[39m comp = D.Independent(D.Normal(loc=mdn_mu_bip, scale=mdn_sigma_bip), \u001b[32m1\u001b[39m) \u001b[38;5;66;03m# Reinterpret batch dim as event dim\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/285_proj_kernel/lib/python3.11/site-packages/torch/distributions/categorical.py:72\u001b[39m, in \u001b[36mCategorical.__init__\u001b[39m\u001b[34m(self, probs, logits, validate_args)\u001b[39m\n\u001b[32m     68\u001b[39m \u001b[38;5;28mself\u001b[39m._num_events = \u001b[38;5;28mself\u001b[39m._param.size()[-\u001b[32m1\u001b[39m]\n\u001b[32m     69\u001b[39m batch_shape = (\n\u001b[32m     70\u001b[39m     \u001b[38;5;28mself\u001b[39m._param.size()[:-\u001b[32m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._param.ndimension() > \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m torch.Size()\n\u001b[32m     71\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/285_proj_kernel/lib/python3.11/site-packages/torch/distributions/distribution.py:71\u001b[39m, in \u001b[36mDistribution.__init__\u001b[39m\u001b[34m(self, batch_shape, event_shape, validate_args)\u001b[39m\n\u001b[32m     69\u001b[39m         valid = constraint.check(value)\n\u001b[32m     70\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid.all():\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     72\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     73\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value.shape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     74\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mof distribution \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     75\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mto satisfy the constraint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(constraint)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     76\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     77\u001b[39m             )\n\u001b[32m     78\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n",
      "\u001b[31mValueError\u001b[39m: Expected parameter logits (Tensor of shape (41, 5)) of distribution Categorical(logits: torch.Size([41, 5])) to satisfy the constraint IndependentConstraint(Real(), 1), but found invalid values:\ntensor([[nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan]], device='cuda:0')"
     ]
    }
   ],
   "source": [
    "# Define number of actions based on mapping\n",
    "num_actions = 4 # Take, S&M, Foul, BIP\n",
    "\n",
    "# Define other model params based on choices above\n",
    "model_constructor_params = {\n",
    "    'd_model': 128,\n",
    "    'nhead': 8,\n",
    "    'num_encoder_layers': 4,\n",
    "    'dim_feedforward': 512,\n",
    "    'dropout': 0.1,\n",
    "    'num_actions': num_actions,\n",
    "    'num_mdn_components': 5,\n",
    "    # seq_length will be passed separately\n",
    "    # feature dims/indices/vocabs determined after data loading\n",
    "}\n",
    "\n",
    "run_training(\n",
    "    start_year=2022, # Shortened range for faster testing\n",
    "    cache_dir=\"dataset\",\n",
    "    end_year=2023,\n",
    "    seq_length=10,\n",
    "    model_params=model_constructor_params, # Pass dict here (will be updated internally)\n",
    "    batch_size=128, # Increased batch size\n",
    "    num_epochs=5, # Reduced epochs for faster testing\n",
    "    learning_rate=5e-5, # Adjusted learning rate\n",
    "    device_str='cuda' if torch.cuda.is_available() else 'cpu'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76139b4-75e5-499c-817d-f592875704c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "285_proj_kernel",
   "language": "python",
   "name": "285_proj_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
